# sweep.yaml (Fixed for save_dir error and scheduler warning)
program: train_ctm.py
method: random  # Using random search. This is simple and effective.
metric:
  name: val_dice_score
  goal: maximize

parameters:
  # --- Hyperparameters to Sweep ---
  lr:
    distribution: log_uniform_values
    min: 1e-5
    max: 5e-4
  alpha:
    distribution: uniform
    min: 0.5
    max: 4.0
  lambda_cycle:
    distribution: uniform
    min: 0.5
    max: 2.0
  lambda_expansion:
    distribution: uniform
    min: -0.8
    max: -0.1

# --- How to run your script ---
command:
  - ${env}           # Environment variables
  - torchrun         # Your multi-GPU launcher
  - --nproc_per_node=4
  - ${program}       # Calls 'train_ctm.py'
  - ${args}          # Passes the swept parameters (e.g., --lr=...)
  
  # --- ALL STATIC (non-swept) arguments go here ---
  - --data_dir
  - "/mnt/hot/public/Akul/exhale_pred_data_10pct"
  
  # --- THIS IS THE FIX ---
  # We add a dummy save_dir. Your Python code will overwrite this.
  - --save_dir
  - "wandb_save" 
  # ---------------------

  - --epochs
  - "50"
  - --batch_size
  - "4"
  - --n_workers
  - "8"
  - --val_interval
  - "1"
  - --dataset_fraction
  - "1"